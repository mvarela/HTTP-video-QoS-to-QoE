#+begin_src emacs-lisp :results none 
   (require 'ob-sh)
   (require 'ob-ruby)
   (require 'ob-gnuplot)
#+end_src


* Mapping QoS measurements to QoE estimations for HTTP video streams
** Non-adaptive case
Data collected via dashsimu
*** DONE Data cleanup
CLOSED: [2015-08-12 Wed 13:01]

Configuration parameters.
(TODO: figure what is the relevant output from dashsimu in the output file, and append to this line where needed)
#+begin_src sh :results none
chmod +x *rb
echo "id, movie, segment_length, buffer_size, adaptation, representation, max_br, lr, mlbs, delay, jitter, bandwidth, avg_response_t, avg_throughput, buffer_underrruns, stall_time, initial_delay, total_time" > results.txt
for i in `find . -iname '*xml'`; do
       DIR=`dirname $i`
        echo `echo $i/*xml | sed -e 's,^.*/,,' -e 's/_/, /g' -e 's/\.xml//'`, `./process_dashsimu_output.rb $DIR` >> results.txt;
        ./process_stalls_output.rb "$DIR" ;
        ./overall_qos.rb $DIR;
       for j in `find $DIR -iname 'avera*'`; do
         awk '{if($29 > 0) print $2, $29*100.0}' < $j | sed '/time/d' > $DIR/loss_events.txt;
         pushd $DIR
#Only plot if there have been some losses or stalls
         S1=`wc - l loss_events.txt | awk '{print $1}'`
         S2=`wc - l stalls_plot.txt | awk '{print $1}'`
         if [ "$S1" > "0" -o "$S2" > "2" ] 
           then gnuplot < ../../plot_losses_stalls.gnup > "losses_v_stalls_"`basename $i xml`png;
         fi
         popd
       done

      
done
#+end_src




The =process_dashsimu_output.rb= script allows us to  calculate the overall results file for the whole run
#+begin_src ruby :tangle process_dashsimu_output.rb :results none :exports code :padline no
#!/usr/bin/env ruby
dname = ARGV[0]
Dir.chdir dname
# Read the file and discard header
line = (File.readlines "dashsimu_output.txt").select{|x| x=~/INFO/}[0].gsub!(/^.*-/,"").chomp.split
# line format is: Avg resp time, Avg throughput, Number of segments, Buffer underruns, Stall time, Initial Delay, Total time
# not interested in the number of segments, so we drop it
line.delete_at 2
results = (line.join ", ")
STDOUT.printf("%s\n", results)
#+end_src


The =process_stalls_output.rb= parses the playback log of dashsimu, and extracts the stall events
#+begin_src ruby :tangle process_stalls_output.rb :results none :exports code :padline no 
#!/usr/bin/env ruby
dname = ARGV[0]
Dir.chdir dname
logfile = Dir.glob("play*log")[0]
lines = File.readlines(logfile)
outf  = File.open("stalls.csv", "w")
plotf = File.open("stalls_plot.txt" ,"w")
lines.select!{|l| l=~/(Stall|Playback) starts/}
#skip the first "Playback starts" line, as it indicates the beginning of playback, not a stall
if(lines.size < 1) then exit end
lines.shift 
outf.printf("stall_start, playback_start, relative_stall, relative_playback, stall_duration\n") 
while(lines.size > 1) do
    stall = lines.shift.sub(",",".").gsub("-"," ").gsub(":"," ").chomp.split" "
    stall_start_time = Time.new stall[0], stall[1], stall[2], stall[3], stall[4], stall[5].to_f
    stall_relative_start_time = stall[-1].to_f
    playback = lines.shift.sub(",",".").gsub("-"," ").gsub(":"," ").chomp.split" "
    playback_start_time = Time.new playback[0], playback[1], playback[2], playback[3], playback[4], playback[5].to_f
    playback_relative_start_time = playback[-1].to_f
    duration = playback_relative_start_time - stall_relative_start_time
    outf.printf("%s, %s, %f, %f, %f\n", stall_start_time, playback_start_time, stall_relative_start_time, playback_relative_start_time, duration)
    plotf.printf("%f, 0\n %f, 1\n", stall_start_time.to_f, stall_start_time.to_f)
    plotf.printf("%f, 1\n %f, 0\n", playback_start_time.to_f, playback_start_time.to_f)
end
outf.close
plotf.close
#+end_src

The =overall_qos.rb= script extracts the LR, MLBS and average throughput for each measurement, which is then to be fed to the QoS-to-buffer model.

#+begin_src ruby :tangle overall_qos.rb :results none :exports code :padline no
#!/usr/bin/env ruby

# Read the Qosmet output, and discard the header lines at the top (19)
dname = ARGV[0]
Dir.chdir dname
fname = Dir.glob("averages*")[0]
lines = File.readlines(fname).drop 19

# We are interested only in some DL columns: total LR (30), total packets (33), Connection break count (43), and Load (32)
# for the Connection break count we need the sum, and for the Load the average
sum_cb = 0
sum_l  = 0
lines.each_index do |i|
  lines[i] = lines[i].split 
  sum_cb  += lines[i][42].to_f
  sum_l   += lines[i][30].to_f
end
if(0 == sum_cb) then
  sum_cb = 1
end
n = lines.size
average_load = sum_l / n
total_lr = lines[n - 1][29].to_f
total_packets = lines[n - 1][32].to_f
lost_packets = (total_packets * total_lr).ceil
mlbs = lost_packets / sum_cb
if(0<mlbs && mlbs<1) then 
  STDERR.printf("---> #{dname}\nSomething's fishy here... MLBS < 1\n")
end
fout = File.open("overall_qos.csv", "w")
fout.printf("LR, MLBS, Throughput\n")
fout.printf("%.5f, %.5f, %.5f\n", total_lr, mlbs, average_load)
fout.close
#+end_src

In order to print the loss and stall events, we use the following Gnuplot snippet

#+begin_src gnuplot :tangle plot_losses_stalls.gnup :results none :exports code :padline no
set terminal png size 900,400
set title "Loss events vs. Stall events"
set grid x y
set xlabel "Time"
set ylabel "Stalls"
set xdata time
set yrange [0:20]
set timefmt "%s"
set format x "%H:%M:%S"
set key left top
plot 'loss_events.txt' using 1:2 with lines title 'LR over 1s period (%)', 'stalls_plot.txt' using 1:2 with lines title 'Stalls events'  
#+end_src


*** Short-term prediction of stalls

As a first step towards understanding how the QoS impacts the stall events, we
will look at the temporal correlation between loss events and the start of a
stall. We will start with a very simple analysis, looking, for each loss rate
observed in a 1s window, whether we observe stall events in the next /S/
seconds, and if so, with which delay. We will therefore output a probability
distribution for stalls happening during the /S/ seconds following a loss event. 


#+begin_src ruby :tangle distances.rb :results none :exports code :padline no
#!/usr/bin/env ruby

require 'json'

def insert_distance(distances, lr, d)
  if(!distances.has_key?(lr))
    distances[lr] = Array.new(S,0)
  end
  distances[lr][d - 1] += 1
end

# Number of seconds to look at before a stall event
S = 10

distances = {}

dname = ARGV[0]
Dir.chdir dname

# If the file contains only the header, we bail
stalls = File.readlines 'stalls_plot.txt'
exit unless(stalls.size > 0) 

# The first two lines are not useful, if there
# is nothing else, we also bail
losses = File.readlines 'loss_events.txt'
exit unless(losses.size > 2)
losses.shift 2

# Collect start times for stalls: every fourth line, starting from the first
stall_start_times = []
stalls.each_index do |i|
  if(0 == i % 4)
    stall_start_times << stalls[i].split(',')[0].to_f
  end
end

loss_events = []
losses.each do |l|
  loss_events << l.split.map(&:to_f)
end
stall_start_times.each do |s|
  # last loss event before stall
  ll_index = loss_events.rindex{ |l| l[0]< s }
  break unless ll_index
  ts = loss_events[ll_index][0]
  # Get all "first loss" events up to S seconds before the stall
  while((ts > 0) &&  (ts >= (s - S)) && (ll_index >= 0))
    # check whether this was the first monitoring period with a loss,
    # if not, try the previous one
    if(ts - loss_events[ll_index-1][0] > 1.1)
      loss_rate = loss_events[ll_index][1].round
      ellapsed = (s - ts).round
      insert_distance(distances, loss_rate, ellapsed)
    end
    ll_index -= 1
    ts = loss_events[ll_index][0]
  end
end

if(distances.size > 0)
config =  ARGV[0].gsub(/^.*\//,"")
out = {:config => config, :distances => distances}
STDOUT.printf("%s\n", out.to_json)
end
#+end_src


We can do a first run of this collecting the data for configurations with the
full bandwidth:


#+begin_src sh :results none
rm out_full_bw.txt > /dev/null 2&>1
for i in `find 2015_08_31 -type d -iname '*15000*'`; do ./distances.rb $i >> out_full_bw.txt; done
#+end_src

We will then need to collect the data from each configuration and aggregate it,
which is done as follows:


#+begin_src ruby :tangle aggregate_distances.rb :results none :exports code :padline no
#!/usr/bin/env ruby

require 'json'
require 'ascii_charts'

# A small auxiliary function, because Ruby's library ain't as
# neat as the Haskell prelude yet...
def zip_with(a, b, &op)
  result = []
  a.zip(b){ |aa,bb| result << op.call(aa,bb)}
  result
end

# We will create a hash containing, for each loss rate observed, the observed
# distribution of stall starts over the /S/ seconds that follow
aggregates = {}

# read a file given as an argument, containing the JSON output by the
# "distances" script, and aggregate the data therein. At this first instance, we
# will not consider the different configuration parameters, and just lump everything
# together, to see what the data looks like.

lines = File.readlines ARGV[0]
lines.each do |l| 
  data = JSON.parse l
  d    = data["distances"]
  d.each do |k,v|
    if(!aggregates.has_key?(k.to_i))
      aggregates[k.to_i] = Array.new(v.size, 0)
    end
    aggregates[k.to_i] = zip_with(aggregates[k.to_i], v, &:+)
  end
end

p aggregates.keys.sort

aggregates.keys.sort.each do |k|
  v = aggregates[k]
  distribution = (1..v.size).to_a.zip(v)
  printf("LR = %d\n", k)
  printf("Distribution = %s\n", distribution)
  puts AsciiCharts::Cartesian.new(distribution, :bar => true, :hide_zero => false).draw
end
#+end_src

*** Stall distribution tests
We would like to know, given the LR and MLBS, and available bandwidth observed during a
period, what is the probability distribution for the number of stalls in the
period, as well as the average stall time. 

We start from the =results.txt= summary generated above (over suitable data,
with several repetitions per configuration), and extract for each, the number of
stalls and the average stall time (which we will then put in buckets).

#+begin_src sh :results none
rm results_qos_histograms.txt > /dev/null 2&>1
for i in qos_histograms/{0,1,2,3,4,5,6,7,8,9}*; do 
        echo $i  `./process_dashsimu_output.rb $i` >> results_qos_histograms.txt; ./build_qos_histograms.rb < results_qos_histograms.txt;
done
#+end_src


 We then need to count, for each triple of (LR,MLBS,BW), the observed
 number of stalls, and the observed average stall duration, and save the info as
 histograms for later learning. 

#+begin_src ruby :tangle build_qos_histograms.rb :results none :exports code :padline no
#!/usr/bin/env ruby

# we get the input from STDIN, looks like:
# qos_histograms/436_transformers_2_2_False_2_0_15_1.25_0_0_15000_491 15.9512601296, 1173362, 9, 394.302549124, 25.6443109512, 479.38789301
# We need to get the configuration info, the number of stalls, and the total stall time, which we then use to calculate the average stall time.

results = {}
MAX_STALLS = 10
MAX_AVG_STALL_DURATION = 90
BUCKET_SIZE = 5
STDIN.each_line do |line|
  l              = line.split
  conf_data      = l[0]
  stall_count    = l[3].to_i
  stall_duration     = l[4].to_f
  avg_stall_time = 0
  if(stall_count > 0) then
    avg_stall_time = (stall_duration / stall_count).ceil
  end
  conf = conf_data.split "_"
  lr   = conf[8]
  mlbs = conf[9]
  bw   = conf[12]

  key = lr + ", " + mlbs + ", " + bw
  if(!results.has_key?(key)) then
    results[key] = {}
    results[key][:stall_count] = Array.new(MAX_STALLS + 1, 0)
    results[key][:stall_duration] = Array.new((MAX_AVG_STALL_DURATION / BUCKET_SIZE) + 1, 0)
  end
  case stall_count
  when 0..(MAX_STALLS - 1)
    results[key][:stall_count][stall_count] += 1
  else
    results[key][:stall_count][MAX_STALLS] += 1
  end

  case avg_stall_time
  when 0..(MAX_AVG_STALL_DURATION-1)
    results[key][:stall_duration][avg_stall_time.to_i/BUCKET_SIZE] += 1
  else
    results[key][:stall_duration][MAX_AVG_STALL_DURATION/BUCKET_SIZE] += 1
  end
end


fout_h = File.open "stall_histograms.txt", "w"
fout_d = File.open "stall_duration_histograms.txt", "w"

fout_h.printf("#LR, MLBS, BW, stall# 0..#{MAX_STALLS - 1},over_limit\n")
fout_d.printf("#LR, MLBS, BW, avg_stall_duration 0..#{MAX_STALLS - 1}/#{BUCKET_SIZE},over_limit\n")

results.keys.sort.each do |k|
  sc = results[k][:stall_count]
  sum_c = sc.reduce(:+)

  sd = results[k][:stall_duration]
  sum_d = sd.reduce(:+)

  fout_h.printf("%s, %s\n", k, sc.map{ |x| (x.to_f/sum_c).to_s}.join(", "))
  fout_d.printf("%s, %s\n", k, results[k][:stall_duration].map{ |x| (x.to_f/sum_d).to_s}.join(", "))
end

fout_h.close
fout_d.close
#+end_src

A different measurement campaign was done for validation purposes (i.e., to test
the models created with the data collected above). 

We can process the validation data using the same scripts as before.


#+begin_src sh :results none
rm validation_results_qos_histograms.txt > /dev/null 2&>1
for i in qos_histograms-validation/{0,1,2,3,4}*; do 
        echo $i  `./process_dashsimu_output.rb $i` >> validation_results_qos_histograms.txt; ./build_qos_histograms.rb < validation_results_qos_histograms.txt;
done
#+end_src
